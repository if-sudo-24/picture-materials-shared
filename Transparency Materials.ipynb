{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our research falls within the category of quantitative studies, we have adhered to the MISQ's guidelines for this category to organize the transparency materials.\n",
    "\n",
    "# Data collection procedures and materials\n",
    "In this study, the quantitative data, including secondary sources such as online review content and reviewer information, were obtained directly from the enterprise (dianping.com) through a cooperative agreement. The data provided by the enterprise were already organized and presented in a structured format. Consequently, no further processing was required for the purposes of this research. Therefore, additional details regarding data collection procedures and materials are not necessary in this article.\n",
    "\n",
    "Although it is not necessary for us to provide detailed data collection procedures and materials, we can offer metadata about the dataset to assist readers in understanding and referencing the data. This metadata includes information such as the timeframe of data collection, the types of data collected (e.g., review content, reviewer profiles), the structure and format of the dataset.\n",
    "\n",
    "By providing this metadata, we aim to enhance the transparency and reproducibility of the study, enabling readers to better comprehend the scope and characteristics of the data used, even in the absence of direct access to the dataset itself.\n",
    "\n",
    "# Data\n",
    "The data used in this study cannot be provided due to the following reasons:\n",
    "\n",
    "1. Sensitivity of Individual Data: The dataset contains highly sensitive information related to individual users, which raises significant privacy concerns.\n",
    "2. Non-Disclosure Agreement (NDA): Access to the data was granted under the terms of a strict non-disclosure agreement with the enterprise, which prohibits the sharing or redistribution of the dataset.\n",
    "\n",
    "As an alternative, we will outline how readers can obtain the same or similar data. Specifically, readers can access comparable datasets by visiting the enterprise’s official website (dianping.com). The platform provides publicly available information, such as online review content and reviewer profiles, which may serve as a valuable substitute for the data utilized in this study. \n",
    "\n",
    "To access the same or similar data, readers or users must register and become members of the enterprise’s platform, dianping.com. As registered members, they can browse the data available on the platform at the current point in time. However, the right to browse the data is not equivalent to the right to collect and use the data. The readers should respect all rules or legal obligations offered by the platform if they want to collect its data.\n",
    "\n",
    "It should be noted, however, that some data included in this study’s dataset (from 2015 to 2017) may no longer be available on the platform due to deletions by the enterprise or individual users. While these deleted records are no longer accessible, they were relevant and valuable to the research conducted in this study and were therefore included in the analysis.\n",
    "\n",
    "Despite these limitations, the proposed method provides sufficient access to comparable data for readers seeking to explore similar research questions or verify the findings of this study.\n",
    "\n",
    "# Data analytic methods\n",
    "In this study, we employ a nonlinear multilevel hierarchical model for our empirical research. The primary methodological references for this approach are drawn from articles published in the R Journal. While we are unable to disclose the actual dataset used due to confidentiality constraints, we ensure the transparency of our research by making the model and corresponding (pseudo)code publicly available. This approach not only facilitates reproducibility but also provides a framework for researchers to adapt and apply similar methodologies to their respective datasets.\n",
    "\n",
    "The use of a nonlinear multilevel hierarchical model is particularly suitable for capturing the complex, nested structure of our data, as well as for accommodating potential nonlinear relationships. By referencing state-of-the-art techniques documented in the R Journal (all necessary references have been included in the manuscript), we align our work with established best practices in statistical modeling and computation. The pseudocode provided aims to offer clear and detailed guidance on the implementation of the model, thereby enabling other researchers to replicate our analytical framework or extend it to new contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative primary data\n",
    "First, we present the processes and code used for reading experimental data, conducting balance checks, and calculating reliability and validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import (\n",
    "    ConfirmatoryFactorAnalyzer, ModelSpecificationParser)\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.mediation import Mediation\n",
    "import statsmodels.api as sm\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(158)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "# usage: df.progress_apply()\n",
    "\n",
    "# %% load data\n",
    "df = pd.read_csv('exdata.csv', header=0, encoding='gbk')\n",
    "df = df.dropna()\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.columns)\n",
    "'''\n",
    "Index(['group', 'gender', 'age', 'job', 'city', 'education', 'salary',\n",
    "       'satiety1', 'tval', 'taro', 'pval', 'paro', 'pf1-', 'pf2', 'pf3',\n",
    "       'pf4-', 'pf5', 'ce1', 'ce2', 'ce3', 'a1', 'a2', 'a3', 'a4', 'pa', 'wr1',    \n",
    "       'wr2', 'wr3', 'app', 'avo', 'phea', 'ppal', 'ph1', 'ph2', 'ph3', 'pi1',     \n",
    "       'pi2', 'pi3', 'tval_sr', 'taro_sr', 'pval_sr', 'paro_sr', 'pf1-_sr',        \n",
    "       'pf2_sr', 'pf3_sr', 'pf4-_sr', 'pf5_sr', 'ce1_sr', 'ce2_sr', 'ce3_sr',      \n",
    "       'a1_sr', 'a2_sr', 'a3_sr', 'a4_sr', 'pa_sr', 'wr1_sr', 'wr2_sr',\n",
    "       'wr3_sr', 'app_sr', 'avo_sr', 'phea_sr', 'ppal_sr', 'ph1_sr', 'ph2_sr',     \n",
    "       'ph3_sr', 'pi1_sr', 'pi2_sr', 'pi3_sr', 'satiety2', 'id_p', 'id_i'],        \n",
    "      dtype='object')\n",
    "'''\n",
    "\n",
    "df1 = pd.read_csv('data1.csv', header=0)\n",
    "print(df1.columns)\n",
    "df1['PicAro2'] = df1['PicAro'] * df1['PicAro']\n",
    "df1['TextAro2'] = df1['TextAro'] * df1['TextAro']\n",
    "mod = sm.OLS.from_formula(\n",
    "    \"Helpfulness ~ 1 + PicVal + PicAro + PicAro2 + TextVal + TextAro + TextAro2 + C(SP) + C(AT) + C(SU)\", df1)\n",
    "print(mod.fit().summary())\n",
    "\n",
    "\n",
    "# %% basic information\n",
    "print(df.groupby('gender').size())\n",
    "'''\n",
    "gender\n",
    "女    230\n",
    "男    192\n",
    "'''\n",
    "print(df.groupby('age').size())\n",
    "'''\n",
    "age\n",
    "20岁及以下     83\n",
    "21-39岁    289\n",
    "40岁及以上     50\n",
    "'''\n",
    "print(df.groupby('salary').size())\n",
    "'''\n",
    "salary\n",
    "20万及以上    101\n",
    "5万-20万    108\n",
    "5万及以下     213\n",
    "'''\n",
    "print(df.groupby('education').size())\n",
    "'''\n",
    "education\n",
    "专科或职业教育     38\n",
    "本科         215\n",
    "研究生及以上     151\n",
    "高中及以下       18\n",
    "'''\n",
    "\n",
    "# %% balance check\n",
    "df.groupby('group')['gender'].value_counts()\n",
    "df.groupby('group')['age'].value_counts()\n",
    "df.groupby('group')['salary'].value_counts()\n",
    "df.groupby('group')['education'].value_counts()\n",
    "cross_tab = {attri: pd.crosstab(df['group'], df[attri])\n",
    "             for attri in ['gender', 'age', 'salary', 'education']}\n",
    "balance_tab = pd.concat(cross_tab.values(), axis=1)\n",
    "\n",
    "chisq_res = {attri: chi2_contingency(cross_tab[attri])\n",
    "             for attri in ['gender', 'age', 'salary', 'education']}\n",
    "# print(chisq_res)\n",
    "'''\n",
    "'gender': (18.16703216998806, 0.2539860477044757, 15)\n",
    "'age': (30.217701330671503, 0.45454521350793303, 30)\n",
    "'salary': (36.24127531879915, 0.20028380890079578, 30)\n",
    "'education': (43.03444512476223, 0.5555432275471534, 45)\n",
    "'''\n",
    "\n",
    "# %% Cronbach's alphas (reliability)\n",
    "\n",
    "\n",
    "def C_alpha(var_frame):\n",
    "    cors = var_frame.corr()\n",
    "    # number of questions\n",
    "    N = cors.shape[0]\n",
    "    # 2.2 Calculate R\n",
    "    # For this, we'll loop through the columns and append every\n",
    "    # relevant correlation to an array calles \"r_s\". Then, we'll\n",
    "    # calculate the mean of \"r_s\"\n",
    "    rs = np.array([])\n",
    "    for i, col in enumerate(cors.columns):\n",
    "        sum_ = cors[col][i+1:].values\n",
    "        rs = np.append(sum_, rs)\n",
    "    mean_r = np.mean(rs)\n",
    "   # 3. Use the formula to calculate Cronbach's Alpha\n",
    "    cronbach_alpha = (N * mean_r) / (1 + (N - 1) * mean_r)\n",
    "    return cronbach_alpha\n",
    "\n",
    "\n",
    "# for the 1st review\n",
    "df['pf1'] = 7 - df['pf1-']\n",
    "df['pf4'] = 7 - df['pf4-']\n",
    "# print(C_alpha(df[['pf1','pf2','pf3','pf4','pf5']]))\n",
    "# print(C_alpha(df[['pf2','pf3','pf5']]))\n",
    "# print(C_alpha(df[['ce1','ce2','ce3']]))\n",
    "# print(C_alpha(df[['a1','a2','a3','a4']]))\n",
    "# print(C_alpha(df[['wr1','wr2','wr3']]))\n",
    "# print(C_alpha(df[['ph1','ph2','ph3']]))\n",
    "# print(C_alpha(df[['pi1','pi2','pi3']]))\n",
    "'''\n",
    "0.8591707541611933\n",
    "0.8860333878166867\n",
    "0.8807747263608968\n",
    "0.8828780218307613\n",
    "0.8892377878815275\n",
    "0.9197336378508992\n",
    "0.9696365372971941\n",
    "'''\n",
    "\n",
    "# for the 2nd review\n",
    "df['pf1_sr'] = 7 - df['pf1-_sr']\n",
    "df['pf4_sr'] = 7 - df['pf4-_sr']\n",
    "# print(C_alpha(df[['pf1_sr','pf2_sr','pf3_sr','pf4_sr','pf5_sr']]))\n",
    "# print(C_alpha(df[['pf2_sr','pf3_sr','pf5_sr']]))\n",
    "# print(C_alpha(df[['ce1_sr','ce2_sr','ce3_sr']]))\n",
    "# print(C_alpha(df[['a1_sr','a2_sr','a3_sr','a4_sr']]))\n",
    "# print(C_alpha(df[['wr1_sr','wr2_sr','wr3_sr']]))\n",
    "# print(C_alpha(df[['ph1_sr','ph2_sr','ph3_sr']]))\n",
    "# print(C_alpha(df[['pi1_sr','pi2_sr','pi3_sr']]))\n",
    "'''\n",
    "0.9090818090518659\n",
    "0.9146989257990688\n",
    "0.9133944720786907\n",
    "0.8779809460605016\n",
    "0.8944337215512985\n",
    "0.936490351776063\n",
    "0.979782955766911\n",
    "'''\n",
    "\n",
    "# %% manipulation check 1\n",
    "pic_df = df[df['group'].apply(\n",
    "    lambda x: x in [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15])]\n",
    "tex_df = df[df['group'].apply(lambda x: x in [4, 8, 12, 16])]\n",
    "\n",
    "tv, pv = stats.ttest_ind(df[df['tvt1'] == 1]['tval'],\n",
    "                         df[df['group'] >= 13]['tval'])\n",
    "print(str(pv/2))  # one-tailed t-test\n",
    "print(df[df['tvt1'] == 1]['tval'].mean())\n",
    "print(df[df['group'] >= 13]['tval'].mean())\n",
    "\n",
    "tv, pv = stats.ttest_ind(df[df['tvt1'] == 1]['tval'],\n",
    "                         df[df['tvt1'] == 0]['taro'])\n",
    "print(\"wtf\" + str(pv/2))  # one-tailed t-test\n",
    "\n",
    "\n",
    "tv, pv = stats.ttest_ind(df[df['tvt1'] == 1]['taro'],\n",
    "                         df[df['tvt1'] == 0]['taro'])\n",
    "print(str(pv/2))  # one-tailed t-test\n",
    "print(df[df['tvt1'] == 1]['taro'].mean())\n",
    "print(df[df['tvt1'] == 0]['taro'].mean())\n",
    "\n",
    "tv, pv = stats.ttest_ind(pic_df[pic_df['pt1'] == 'G']['pval'],\n",
    "                         pic_df[pic_df['pt1'] == 'R']['pval'])\n",
    "print(str(pv/2))  # one-tailed t-test\n",
    "print(pic_df[pic_df['pt1'] == 'G']['pval'].mean())\n",
    "print(pic_df[pic_df['pt1'] == 'R']['pval'].mean())\n",
    "\n",
    "tv, pv = stats.ttest_ind(pic_df[pic_df['pt1'] == 'G']['paro'],\n",
    "                         pic_df[pic_df['pt1'] == 'R']['paro'],)\n",
    "print(str(pv/2))  # one-tailed t-test\n",
    "print(pic_df[pic_df['pt1'] == 'G']['paro'].mean())\n",
    "print(pic_df[pic_df['pt1'] == 'R']['paro'].mean())\n",
    "\n",
    "tv, pv = stats.ttest_ind(pic_df[pic_df['pt1'] == 'G']['pval'],\n",
    "                         pic_df[pic_df['pt1'] == 'G']['paro'],)\n",
    "print(\"wtf\" + str(pv/2))  # one-tailed t-test\n",
    "\n",
    "# %% manipulation check 2\n",
    "tv, pv = stats.ttest_ind(pic_df[pic_df['pt1'] == 'G']['ppal'],\n",
    "                         pic_df[pic_df['pt1'] == 'O']['ppal'])\n",
    "print(str(pv/2))  # one-tailed t-test\n",
    "print(pic_df[pic_df['pt1'] == 'G']['ppal'].mean())\n",
    "print(pic_df[pic_df['pt1'] == 'O']['ppal'].mean())\n",
    "\n",
    "tv, pv = stats.ttest_ind(pic_df[pic_df['pt1'] == 'G']['phea'],\n",
    "                         pic_df[pic_df['pt1'] == 'O']['phea'])\n",
    "print(str(pv/2))  # one-tailed t-test\n",
    "print(pic_df[pic_df['pt1'] == 'G']['phea'].mean())\n",
    "print(pic_df[pic_df['pt1'] == 'O']['phea'].mean())\n",
    "\n",
    "tv, pv = stats.ttest_ind(pic_df['taro'],\n",
    "                         pic_df['paro'])\n",
    "print(str(pv/2))  # one-tailed t-test\n",
    "print(pic_df['taro'].mean())\n",
    "print(pic_df['paro'].mean())\n",
    "\n",
    "\n",
    "# %% Confirmatory factor analysis (validity) without pf1, pf4\n",
    "temp = df[['pf2', 'pf3',  'pf5', 'ce1', 'ce2', 'ce3', 'a1', 'a2',\n",
    "           'a3', 'a4', 'wr1', 'wr2', 'wr3', 'ph1', 'ph2', 'ph3', 'pi1', 'pi2', 'pi3']]\n",
    "model_dict = {\"F1\": ['pf2', 'pf3', 'pf5'],\n",
    "              \"F2\": ['ce1', 'ce2', 'ce3'],\n",
    "              \"F3\": ['a1', 'a2', 'a3', 'a4'],\n",
    "              \"F4\": ['wr1', 'wr2', 'wr3'],\n",
    "              \"F5\": ['ph1', 'ph2', 'ph3'],\n",
    "              \"F6\": ['pi1', 'pi2', 'pi3']}\n",
    "model_spec = ModelSpecificationParser.parse_model_specification_from_dict(\n",
    "    temp, model_dict)\n",
    "cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n",
    "cfa.fit(temp.values)\n",
    "loadings = cfa.loadings_\n",
    "loadings_se, error_vars_se = cfa.get_standard_errors()\n",
    "# print(loadings_se)\n",
    "'''\n",
    "[[0.05088133 0.         0.         0.         0.         0.        ]\n",
    " [0.05015584 0.         0.         0.         0.         0.        ]\n",
    " [0.05904468 0.         0.         0.         0.         0.        ]\n",
    " [0.         0.05111221 0.         0.         0.         0.        ]\n",
    " [0.         0.05234574 0.         0.         0.         0.        ]\n",
    " [0.         0.05075873 0.         0.         0.         0.        ]\n",
    " [0.         0.         0.05154054 0.         0.         0.        ]\n",
    " [0.         0.         0.04858426 0.         0.         0.        ]\n",
    " [0.         0.         0.05545921 0.         0.         0.        ]\n",
    " [0.         0.         0.05814206 0.         0.         0.        ]\n",
    " [0.         0.         0.         0.05917729 0.         0.        ]\n",
    " [0.         0.         0.         0.05045251 0.         0.        ]\n",
    " [0.         0.         0.         0.05449334 0.         0.        ]\n",
    " [0.         0.         0.         0.         0.04671028 0.        ]\n",
    " [0.         0.         0.         0.         0.04760733 0.        ]\n",
    " [0.         0.         0.         0.         0.04712727 0.        ]\n",
    " [0.         0.         0.         0.         0.         0.04421903]\n",
    " [0.         0.         0.         0.         0.         0.04683588]\n",
    " [0.         0.         0.         0.         0.         0.04727852]]\n",
    "'''\n",
    "error_vars_se = np.sqrt(cfa.error_vars_.reshape(19,))\n",
    "# print(cfa.factor_varcovs_)\n",
    "'''\n",
    "[[1.         0.0675506  0.1388678  0.09007235 0.09670755 0.03067129]\n",
    " [0.0675506  1.         0.29058941 0.26444203 0.29339554 0.1406637 ]\n",
    " [0.1388678  0.29058941 1.         0.37179055 0.35404266 0.1626066 ]\n",
    " [0.09007235 0.26444203 0.37179055 1.         0.34159494 0.16602322]\n",
    " [0.09670755 0.29339554 0.35404266 0.34159494 1.         0.08908992]\n",
    " [0.03067129 0.1406637  0.1626066  0.16602322 0.08908992 1.        ]]\n",
    " '''\n",
    "\n",
    "loads = loadings.reshape(114,)[[0, 6, 12, 19, 25, 31, 38, 44,\n",
    "                                50, 56, 63, 69, 75, 82, 88, 94, 101, 107, 113]]\n",
    "# print(sum(loads[:3]**2)/(sum(loads[:3]**2)+ sum(error_vars_se[:3]**2)))\n",
    "# print(sum(loads[3:6]**2)/(sum(loads[3:6]**2)+ sum(error_vars_se[3:6]**2)))\n",
    "# print(sum(loads[6:10]**2)/(sum(loads[6:10]**2)+ sum(error_vars_se[6:10]**2)))\n",
    "# print(sum(loads[10:13]**2)/(sum(loads[10:13]**2)+ sum(error_vars_se[10:13]**2)))\n",
    "# print(sum(loads[13:16]**2)/(sum(loads[13:16]**2)+ sum(error_vars_se[13:16]**2)))\n",
    "# print(sum(loads[16:19]**2)/(sum(loads[16:19]**2)+ sum(error_vars_se[16:19]**2)))\n",
    "\n",
    "'''\n",
    "0.6804573617420825\n",
    "0.6848158607818193\n",
    "0.633622713634425\n",
    "0.6700579054797045\n",
    "0.7420011115386957\n",
    "0.81761304370953\n",
    "'''\n",
    "\n",
    "# %% Confirmatory factor analysis (validity) with pf1, pf4\n",
    "temp = df[['pf1-', 'pf2', 'pf3', 'pf4-', 'pf5', 'ce1', 'ce2', 'ce3', 'a1', 'a2',\n",
    "           'a3', 'a4', 'wr1', 'wr2', 'wr3', 'ph1', 'ph2', 'ph3', 'pi1', 'pi2', 'pi3']]\n",
    "model_dict = {\"F1\": ['pf1-', 'pf2', 'pf3', 'pf4-', 'pf5'],\n",
    "              \"F2\": ['ce1', 'ce2', 'ce3'],\n",
    "              \"F3\": ['a1', 'a2', 'a3', 'a4'],\n",
    "              \"F4\": ['wr1', 'wr2', 'wr3'],\n",
    "              \"F5\": ['ph1', 'ph2', 'ph3'],\n",
    "              \"F6\": ['pi1', 'pi2', 'pi3']}\n",
    "model_spec = ModelSpecificationParser.parse_model_specification_from_dict(\n",
    "    temp, model_dict)\n",
    "cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n",
    "cfa.fit(temp.values)\n",
    "loadings = cfa.loadings_\n",
    "loadings_se, error_vars_se = cfa.get_standard_errors()\n",
    "# print(loadings_se)\n",
    "'''\n",
    "[[0.07149457 0.         0.         0.         0.         0.        ]\n",
    " [0.05220995 0.         0.         0.         0.         0.        ]\n",
    " [0.05272207 0.         0.         0.         0.         0.        ]\n",
    " [0.06326951 0.         0.         0.         0.         0.        ]\n",
    " [0.05843026 0.         0.         0.         0.         0.        ]\n",
    " [0.         0.05086663 0.         0.         0.         0.        ]\n",
    " [0.         0.05212719 0.         0.         0.         0.        ]\n",
    " [0.         0.05082515 0.         0.         0.         0.        ]\n",
    " [0.         0.         0.05106664 0.         0.         0.        ]\n",
    " [0.         0.         0.04817152 0.         0.         0.        ]\n",
    " [0.         0.         0.05431391 0.         0.         0.        ]\n",
    " [0.         0.         0.05646783 0.         0.         0.        ]\n",
    " [0.         0.         0.         0.05760128 0.         0.        ]\n",
    " [0.         0.         0.         0.05025958 0.         0.        ]\n",
    " [0.         0.         0.         0.05351047 0.         0.        ]\n",
    " [0.         0.         0.         0.         0.04550612 0.        ]\n",
    " [0.         0.         0.         0.         0.04579569 0.        ]\n",
    " [0.         0.         0.         0.         0.04709788 0.        ]\n",
    " [0.         0.         0.         0.         0.         0.04314606]\n",
    " [0.         0.         0.         0.         0.         0.04494906]\n",
    " [0.         0.         0.         0.         0.         0.04553063]]\n",
    "'''\n",
    "# print(error_vars_se)\n",
    "'''\n",
    "[0.10417787 0.05080782 0.05413315 0.0787603  0.06493564 0.0486113\n",
    " 0.05104546 0.04852934 0.04751227 0.04037249 0.05346928 0.06154693\n",
    " 0.06366381 0.04737554 0.05350057 0.03375969 0.0341493  0.03835605\n",
    " 0.02154999 0.02330404 0.0242826 ]\n",
    " '''\n",
    "\n",
    "error_vars_se = np.sqrt(cfa.error_vars_.reshape(21,))\n",
    "# print(cfa.factor_varcovs_)\n",
    "'''\n",
    "[[ 1.          0.03718711  0.09997903  0.04022141  0.07020146 -0.00843205]\n",
    " [ 0.03718711  1.          0.28030712  0.24365414  0.29265758  0.13512102]\n",
    " [ 0.09997903  0.28030712  1.          0.36289666  0.35920525  0.15644118]\n",
    " [ 0.04022141  0.24365414  0.36289666  1.          0.33421972  0.15825606]\n",
    " [ 0.07020146  0.29265758  0.35920525  0.33421972  1.          0.07365902]\n",
    " [-0.00843205  0.13512102  0.15644118  0.15825606  0.07365902  1.        ]]\n",
    " '''\n",
    "\n",
    "loads = loadings.reshape(126,)[[0, 6, 12, 18, 24, 31, 37, 43,\n",
    "                                50, 56, 62, 68, 75, 81, 87, 94, 100, 106, 113, 119, 125]]\n",
    "# 126 = 21 * 6, corresponds to the non-zero term of loadings_se\n",
    "\n",
    "# print(sum(loads[:5]**2)/(sum(loads[:5]**2)+ sum(error_vars_se[:5]**2)))\n",
    "# print(sum(loads[5:8]**2)/(sum(loads[5:8]**2)+ sum(error_vars_se[5:8]**2)))\n",
    "# print(sum(loads[8:12]**2)/(sum(loads[8:12]**2)+ sum(error_vars_se[8:12]**2)))\n",
    "# print(sum(loads[12:15]**2)/(sum(loads[12:15]**2)+ sum(error_vars_se[12:15]**2)))\n",
    "# print(sum(loads[15:18]**2)/(sum(loads[15:18]**2)+ sum(error_vars_se[15:18]**2)))\n",
    "# print(sum(loads[18:21]**2)/(sum(loads[18:21]**2)+ sum(error_vars_se[18:21]**2)))\n",
    "\n",
    "'''\n",
    "0.5585855030001372\n",
    "0.6827916679027678\n",
    "0.639818985285587\n",
    "0.6774083254145898\n",
    "0.7506165612998753\n",
    "0.8491876427762699\n",
    "'''\n",
    "\n",
    "# values above 0.7 are considered very good\n",
    "# the AVE of each of the latent constructs should be higher than the highest squared correlation with any other latent variable.\n",
    "# If that is the case, discriminant validity is established on the construct level.\n",
    "# In our case, the highest squared correlation is 0.35\n",
    "\n",
    "# %% construct mean variables\n",
    "df['pf'] = (df['pf1-'] + df['pf2'] + df['pf3'] + df['pf4-'] + df['pf5']) / 5\n",
    "df['pfn'] = (df['pf2'] + df['pf3'] + df['pf5']) / 3\n",
    "df['ce'] = (df['ce1'] + df['ce2'] + df['ce3']) / 3\n",
    "df['a'] = (df['a1'] + df['a2'] + df['a3'] + df['a4']) / 4\n",
    "df['wr'] = (df['wr1'] + df['wr2'] + df['wr3']) / 3\n",
    "df['ph'] = (df['ph1'] + df['ph2'] + df['ph3']) / 3\n",
    "df['pi'] = (df['pi1'] + df['pi2'] + df['pi3']) / 3\n",
    "df['pic_a'] = 7 - df['pa']\n",
    "df['paro2'] = df['paro'] * df['paro']\n",
    "df['paro3'] = (df['paro']-4) * (df['paro']-4)\n",
    "df['taro2'] = df['taro'] * df['taro']\n",
    "df['pf1_sr'] = 7 - df['pf1-_sr']\n",
    "df['pf4_sr'] = 7 - df['pf4-_sr']\n",
    "df['pf_sr'] = (df['pf1_sr'] + df['pf2_sr'] + df['pf3_sr'] +\n",
    "               df['pf4_sr'] + df['pf5_sr']) / 5\n",
    "df['ce_sr'] = (df['ce1_sr'] + df['ce2_sr'] + df['ce3_sr']) / 3\n",
    "df['a_sr'] = (df['a1_sr'] + df['a2_sr'] + df['a3_sr'] + df['a4_sr']) / 4\n",
    "df['wr_sr'] = (df['wr1_sr'] + df['wr2_sr'] + df['wr3_sr']) / 3\n",
    "df['ph_sr'] = (df['ph1_sr'] + df['ph2_sr'] + df['ph3_sr']) / 3\n",
    "df['pi_sr'] = (df['pi1_sr'] + df['pi2_sr'] + df['pi3_sr']) / 3\n",
    "df['pic_a_sr'] = 7 - df['pa_sr']\n",
    "df['tpv'] = abs(df['tval'] - df['pval'])\n",
    "df['tpa'] = abs(df['taro'] - df['paro'])\n",
    "df['vx'] = df['tval'] * df['pval']\n",
    "df['ax'] = df['taro'] * df['paro']\n",
    "df['Ipa'] = df['pval'] * (df['pval'] < 3).astype(int)\n",
    "df['Ita'] = df['tval'] * (df['tval'] < 3).astype(int)\n",
    "\n",
    "pic_df = df[df['group'].apply(\n",
    "    lambda x: x in [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15])]\n",
    "tex_df = df[df['group'].apply(lambda x: x in [4, 8, 12, 16])]\n",
    "pic_df_nonr = df[df['group'].apply(\n",
    "    lambda x: x in [2, 3, 6, 7, 10, 11, 14, 15])]\n",
    "regs = {}  # a dict to store regression results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we present the processes and code used for reading text pretest data and conducting a manipulation check. The primary goal of this process is to demonstrate that our manipulation of the text achieved the intended effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "data = pd.read_csv(\"Sum.csv\")\n",
    "data.describe()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index(['T', 'UserID', 'UserNo', 'IP Address', 'Started', 'Ended', 'Pid',\n",
    "       'EV1_1', 'EV2_1', 'EV3_1', 'F1', 'F2', 'F3', 'EV1_2', 'EV2_2', 'EV3_2',\n",
    "       'EV1_3', 'EV2_3', 'EV3_3', 'Gender', 'Age', 'Education', 'Income',\n",
    "       'Race', 'Ideology', 'Satiety'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_1 = 1/3 * (data[data[\"T\"] == \"L\"][\"EV1_1\"] + data[data[\"T\"] == \"L\"][\"EV2_1\"] + data[data[\"T\"] == \"L\"][\"EV3_1\"])\n",
    "L_2 = 1/3 * (data[data[\"T\"] == \"L\"][\"EV1_2\"] + data[data[\"T\"] == \"L\"][\"EV2_2\"] + data[data[\"T\"] == \"L\"][\"EV3_2\"])\n",
    "L_3 = 1/3 * (data[data[\"T\"] == \"L\"][\"EV1_3\"] + data[data[\"T\"] == \"L\"][\"EV2_3\"] + data[data[\"T\"] == \"L\"][\"EV3_3\"])\n",
    "\n",
    "B_1 = 1/3 * (data[data[\"T\"] == \"B\"][\"EV1_1\"] + data[data[\"T\"] == \"B\"][\"EV2_1\"] + data[data[\"T\"] == \"B\"][\"EV3_1\"])\n",
    "B_2 = 1/3 * (data[data[\"T\"] == \"B\"][\"EV1_2\"] + data[data[\"T\"] == \"B\"][\"EV2_2\"] + data[data[\"T\"] == \"B\"][\"EV3_2\"])\n",
    "B_3 = 1/3 * (data[data[\"T\"] == \"B\"][\"EV1_3\"] + data[data[\"T\"] == \"B\"][\"EV2_3\"] + data[data[\"T\"] == \"B\"][\"EV3_3\"])\n",
    "\n",
    "H_1 = 1/3 * (data[data[\"T\"] == \"H\"][\"EV1_1\"] + data[data[\"T\"] == \"H\"][\"EV2_1\"] + data[data[\"T\"] == \"H\"][\"EV3_1\"])\n",
    "H_2 = 1/3 * (data[data[\"T\"] == \"H\"][\"EV1_2\"] + data[data[\"T\"] == \"H\"][\"EV2_2\"] + data[data[\"T\"] == \"H\"][\"EV3_2\"])\n",
    "H_3 = 1/3 * (data[data[\"T\"] == \"H\"][\"EV1_3\"] + data[data[\"T\"] == \"H\"][\"EV2_3\"] + data[data[\"T\"] == \"H\"][\"EV3_3\"])\n",
    "\n",
    "VH_1 = 1/3 * (data[data[\"T\"] == \"VH\"][\"EV1_1\"] + data[data[\"T\"] == \"VH\"][\"EV2_1\"] + data[data[\"T\"] == \"VH\"][\"EV3_1\"])\n",
    "VH_2 = 1/3 * (data[data[\"T\"] == \"VH\"][\"EV1_2\"] + data[data[\"T\"] == \"VH\"][\"EV2_2\"] + data[data[\"T\"] == \"VH\"][\"EV3_2\"])\n",
    "VH_3 = 1/3 * (data[data[\"T\"] == \"VH\"][\"EV1_3\"] + data[data[\"T\"] == \"VH\"][\"EV2_3\"] + data[data[\"T\"] == \"VH\"][\"EV3_3\"])\n",
    "\n",
    "t_statistic, p_value = ttest_ind(H_1, VH_1)\n",
    "print(\"%.2f\" % L_1.mean())\n",
    "print(\"%.2f\" % B_1.mean())\n",
    "print(\"%.2f\" % H_1.mean())\n",
    "print(\"%.2f\" % VH_1.mean())\n",
    "p_value = p_value / 2\n",
    "print(\"%.2f\" % p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.60\n",
    "4.69\n",
    "5.72\n",
    "6.21\n",
    "0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "| Review No. | Treatment | Mean | P-value | N |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | VL  | 2.60 | 0.00 | 30 |\n",
    "| 1 | L  | 4.69 | 0.00 | 25 |\n",
    "| 1 | M  | 5.72 | 0.02 | 27 |\n",
    "| 1 | H | 6.21 | - | 34 |\n",
    "| 2 | VL  | 1.68 | 0.00 | 30 |\n",
    "| 2 | L  | 3.44 | 0.00 | 25 |\n",
    "| 2 | M  | 5.81 | 0.08 | 27 |\n",
    "| 2 | H | 6.21 | - | 34 |\n",
    "| 3 | VL  | 1.71 | 0.00 | 30 |\n",
    "| 3 | L  | 4.15 | 0.00 | 25 |\n",
    "| 3 | M  | 5.46 | 0.04 | 27 |\n",
    "| 3 | H | 6.11 | - | 34 |\n",
    "\n",
    "- VL = Very Low valence\n",
    "- L = Low valence\n",
    "- M = Median valence\n",
    "- H = High valence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we present the processes and code used for reading picture pretest data and conducting a manipulation check. The primary goal of this process is to demonstrate that our manipulation of the pictures achieved the intended effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df_v = pd.read_csv('V_data.csv', header = 0)\n",
    "df_a = pd.read_csv('A_data.csv', header = 0)\n",
    "df_o = pd.read_csv('O_data.csv', header = 0)\n",
    "df_v = df_v.dropna()\n",
    "# print(df_v.head())\n",
    "# print(df_v.describe())\n",
    "print(df_v.columns)\n",
    "print(df_a.columns)\n",
    "print(df_o.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index(['Treatment', 'UserID', 'UserNo', 'IP Address', 'Started', 'Ended', 'CC',\n",
    "       'Pid', 'P1v', 'F1v', 'P2v', 'F2v', 'P3v', 'F3v', 'P4v', 'AC1', 'P5v',\n",
    "       'F4v', 'P6v', 'F5v', 'P7v', 'F6v', 'P8v', 'AC2', 'P9v', 'F7v', 'P10v',\n",
    "       'MC', 'Gender', 'Age', 'Education', 'Income', 'Race', 'Ideology',\n",
    "       'Satiety'],\n",
    "      dtype='object')\n",
    "Index(['Treatment', 'UserID', 'UserNo', 'IP Address', 'Started', 'Ended', 'CC',\n",
    "       'Pid', 'P1a', 'F1a', 'P2a', 'F2a', 'P3a', 'F3a', 'P4a', 'AC1', 'P5a',\n",
    "       'F4a', 'P6a', 'F5a', 'P7a', 'F6a', 'P8a', 'AC2', 'P9a', 'F7a', 'P10a',\n",
    "       'MC', 'Gender', 'Age', 'Education', 'Income', 'Race', 'Ideology',\n",
    "       'Satiety'],\n",
    "      dtype='object')\n",
    "Index(['Treatment', 'UserID', 'UserNo', 'IP Address', 'Started', 'Ended', 'CC',\n",
    "       'Pid', 'P1Q', 'P1A1', 'P1A2', 'P1A3', 'P1B1', 'P1B2', 'P1B3', 'P1B4',\n",
    "       'P1B5', 'P2Q', 'P2A1', 'P2A2', 'P2B1', 'P2B2', 'P2B3', 'P3Q', 'P3A1',\n",
    "       'P3A2', 'P3A3', 'P3B1', 'P3B2', 'P3B3', 'P3B4', 'P3B5', 'P4Q', 'P4A1',\n",
    "       'P4A2', 'P4B1', 'P4B2', 'P4B3', 'P5Q', 'P5A1', 'P5A2', 'P5A3', 'P5B1',\n",
    "       'P5B2', 'P5B3', 'P5B4', 'P5B5', 'P6Q', 'P6A1', 'P6A2', 'P6B1', 'P6B2',\n",
    "       'P6B3', 'P7Q', 'P7A1', 'P7A2', 'P7A3', 'P7B1', 'P7B2', 'P7B3', 'P7B4',\n",
    "       'P7B5', 'P8Q', 'P8A1', 'P8A2', 'P8B1', 'P8B2', 'P8B3', 'P9Q', 'P9A1',\n",
    "       'P9A2', 'P9A3', 'P9B1', 'P9B2', 'P9B3', 'P9B4', 'P9B5', 'P10Q', 'P10A1',\n",
    "       'P10A2', 'P10B1', 'P10B2', 'P10B3', 'MC1', 'MC2', 'Gender', 'Age',\n",
    "       'Education', 'Income', 'Race', 'Ideology', 'Satiety'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic, p_value = stats.f_oneway(\n",
    "    df_v['P6v'][df_v['Treatment'] == 'cdp'],\n",
    "    df_v['P6v'][df_v['Treatment'] == 'o']\n",
    ")\n",
    "\n",
    "print(f'F-statistic: {f_statistic}, p-value: {p_value}')\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print('There is a significant difference between at least two treatment groups.')\n",
    "else:\n",
    "    print('No significant difference detected between treatment groups.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-statistic: 3.6959557603686632, p-value: 0.060489765181632134\n",
    "No significant difference detected between treatment groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic, p_value = stats.f_oneway(\n",
    "    df_a['P5v'][df_a['Treatment'] == 'tdp'],\n",
    "    df_a['P5v'][df_a['Treatment'] == 'tdm'],\n",
    "    df_a['P5v'][df_a['Treatment'] == 'o']\n",
    ")\n",
    "\n",
    "print(f'F-statistic: {f_statistic}, p-value: {p_value}')\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print('There is a significant difference between at least two treatment groups.')\n",
    "else:\n",
    "    print('No significant difference detected between treatment groups.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-statistic: 3.579922376213418, p-value: 0.03273874323572236\n",
    "There is a significant difference between at least two treatment groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v[df_v['Treatment'] == 'cdm']['P6v'].mean()\n",
    "tv, pv = stats.ttest_ind(df_a[df_a['Treatment'] == 'cdp']['P7a'], \n",
    "                         df_a[df_a['Treatment'] == 'tdm']['P7a'])\n",
    "print(str(pv/2))\n",
    "print(df_a[df_a['Treatment'] == 'cdm']['P7a'].mean())\n",
    "print(df_a[df_a['Treatment'] == 'cdp']['P7a'].mean())\n",
    "print(df_a[df_a['Treatment'] == 'o']['P7a'].mean())\n",
    "print(df_a[df_a['Treatment'] == 'tdp']['P7a'].mean())\n",
    "print(df_a[df_a['Treatment'] == 'tdm']['P7a'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.861111111111111\n",
    "0.21964264080582696\n",
    "4.162162162162162\n",
    "4.5\n",
    "4.777777777777778\n",
    "4.185185185185185\n",
    "4.791666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv, pv = stats.ttest_ind(df_v[df_v['Treatment'] == 'o']['P7v'], \n",
    "                         df_v[df_v['Treatment'] == 'tdm']['P7v'])\n",
    "print(str(pv/2))\n",
    "print(df_v[df_v['Treatment'] == 'cdm']['P7v'].mean())\n",
    "print(df_v[df_v['Treatment'] == 'cdp']['P7v'].mean())\n",
    "print(df_v[df_v['Treatment'] == 'o']['P7v'].mean())\n",
    "print(df_v[df_v['Treatment'] == 'tdp']['P7v'].mean())\n",
    "print(df_v[df_v['Treatment'] == 'tdm']['P7v'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.13628247215931977\n",
    "5.416666666666667\n",
    "6.076923076923077\n",
    "5.833333333333333\n",
    "5.64\n",
    "5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv, pv = stats.ttest_ind(df_o[df_o['Treatment'] == 'tdm']['P7Q'], \n",
    "                         df_o[df_o['Treatment'] == 'cdp']['P7Q'])\n",
    "print(str(pv/2))\n",
    "print(df_o[df_o['Treatment'] == 'cdm']['P7Q'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'cdp']['P7Q'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'o']['P7Q'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'tdp']['P7Q'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'tdm']['P7Q'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0011914426053452653\n",
    "4.741935483870968\n",
    "5.303030303030303\n",
    "5.363636363636363\n",
    "5.852941176470588\n",
    "4.096774193548387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv, pv = stats.ttest_ind(df_o[df_o['Treatment'] == 'o']['P7A1'], \n",
    "                         df_o[df_o['Treatment'] == 'cdp']['P7A1'])\n",
    "print(str(pv/2))\n",
    "print(df_o[df_o['Treatment'] == 'cdm']['P7A1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'cdp']['P7A1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'o']['P7A1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'tdp']['P7A1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'tdm']['P7A1'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.18957271391492847\n",
    "4.935483870967742\n",
    "5.909090909090909\n",
    "5.590909090909091\n",
    "5.647058823529412\n",
    "4.838709677419355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv, pv = stats.ttest_ind(df_o[df_o['Treatment'] == 'cdp']['P7B1'], \n",
    "                         df_o[df_o['Treatment'] == 'tdm']['P7B1'])\n",
    "print(str(pv/2))\n",
    "print(df_o[df_o['Treatment'] == 'cdm']['P7B1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'cdp']['P7B1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'o']['P7B1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'tdp']['P7B1'].mean())\n",
    "print(df_o[df_o['Treatment'] == 'tdm']['P7B1'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.02383778287183434\n",
    "2.903225806451613\n",
    "3.393939393939394\n",
    "3.6363636363636362\n",
    "3.3529411764705883\n",
    "2.6774193548387095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the code and processes involved in the experiment, please refer to the \\Files\\EX01.ipynb.\n",
    "- For the f-test to show the correlation of PRH and manipulated pictures, please refer to \\Files\\f-test.ipynb.\n",
    "- For the additional ANOVA tests, please refer to \\Files\\EX02.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of Pretests and Main Experiment\n",
    "Due to space limitations, this section provides additional details about the experiments and preliminary trials that could not be included in the main text of the paper.\n",
    "### Pretest on Pictures\n",
    "[Table TM1](\\Files\\TM1.pdf) (TM1.pdf) illustrates the procedure of the first pretest. The protocol follows Kurdi et al. (2016). In addition to our pretest design, we implemented two attention checks to ensure the quality of responses. The first check was a statement addressing decision-making tendencies. Participants were asked to select a specific response and provide a brief explanation if they had carefully read the preceding questions. Another attention check involved the inclusion of a subtly hidden question prompting participants to select a specific response option, “moderately low,” amongst others.\n",
    "At the end of the pretest, we incorporated a memory check, prompting participants to choose one from the four pictures which they had not seen. Moreover, we conducted a check of the balance of the personal information among the groups, and the results showed no imbalance.\n",
    "### Pretest on Text\n",
    "[Table TM2](\\Files\\TM2.pdf) (TM2.pdf) illustrate the procedure of the second pretest. The protocol is adopted from Yin et al. (2021). The same attention checks from the first pretest were also applied to the second pretest. The pretest also passed the balance check, similar to the first pretest.\n",
    "### Main Experiment\n",
    "[Table TM3](\\Files\\TM3.pdf) (TM3.pdf) illustrates the procedure of the main experiment, which is similar to the above pretest. We also incorporate the similar attention checks.\n",
    "\n",
    "## Text-Dic-Report and Img-GPT-Report\n",
    "In this section, we compare the prediction results of our model with those of GPT-3.5. We calculate the differences between the predicted values and the distances between the prediction distributions as evidence of the practical value of our model’s predictions. The code we used is as follows:\n",
    "\n",
    "### Text-Dic-Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# Load the data from both files\n",
    "df_text_train = pd.read_csv('data/text_train.csv')\n",
    "df_moessm1_esm = pd.read_csv('data/MOESM1_ESM.csv')\n",
    "\n",
    "# Extract the list of emotional words from MOESM1_ESM.csv\n",
    "emotional_words = set(df_moessm1_esm['Word'].tolist())\n",
    "\n",
    "# Function to calculate the mean valence and arousal for emotional words in a review\n",
    "def calculate_emotional_means(review, emotional_words, df_moessm1_esm):\n",
    "    # Tokenize the review into words\n",
    "    words = jieba.lcut(review)\n",
    "    # Filter out the emotional words and get their indices in the MOESM1_ESM.csv\n",
    "    emotional_indices = [df_moessm1_esm.index[df_moessm1_esm['Word'] == word][0] for word in words if word in emotional_words]\n",
    "    # Calculate mean valence and arousal\n",
    "    if emotional_indices:\n",
    "        mean_valence = df_moessm1_esm.loc[emotional_indices, 'Valence_Mean'].mean()\n",
    "        mean_arousal = df_moessm1_esm.loc[emotional_indices, 'Arousal_Mean'].mean()\n",
    "    else:\n",
    "        # If there are no emotional words, return 0/2 as the default values for mean valence and arousal\n",
    "        mean_valence = 0\n",
    "        mean_arousal = 2\n",
    "    return mean_valence, mean_arousal\n",
    "\n",
    "# Apply the function to each review in the text_train.csv\n",
    "df_text_train['valence1'], df_text_train['arousal1'] = zip(*df_text_train['review_content'].apply(\n",
    "    lambda x: calculate_emotional_means(x, emotional_words, df_moessm1_esm)))\n",
    "\n",
    "# Save the modified dataframe to a new CSV\n",
    "output_path = 'data/text_train_with_emotional_means.csv'\n",
    "df_text_train.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the values in 'valence1' and 'arousal1' to the range [1, 7] with the specified rules\n",
    "df_text_train_with_emotional_means_mapped['valence2'] = df_text_train_with_emotional_means_mapped['valence1'].apply(lambda x: round(x + 4))\n",
    "df_text_train_with_emotional_means_mapped['arousal2'] = df_text_train_with_emotional_means_mapped['arousal1'].apply(lambda x: round((x + 1) * 1.4))\n",
    "\n",
    "# Save the modified dataframe to a new CSV\n",
    "output_path_mapped_rules = 'data/text_train_with_emotional_means_mapped_rules.csv'\n",
    "df_text_train_with_emotional_means_mapped.to_csv(output_path_mapped_rules, index=False)\n",
    "\n",
    "output_path_mapped_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract 'valence' and 'valence2' columns from the dataframe\n",
    "actual_values = df_text_train_with_emotional_means_modified['valence'].values\n",
    "predicted_values = df_text_train_with_emotional_means_modified['valence2'].values\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = np.mean((actual_values - predicted_values) ** 2)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = np.corrcoef(actual_values, predicted_values)[0, 1] ** 2\n",
    "\n",
    "mse, rmse, r_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = np.mean(np.abs(actual_values - predicted_values))\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((actual_values - predicted_values) / actual_values) * 100)\n",
    "\n",
    "mae, mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Calculate the KS-test statistic and p-value for 'valence' and 'valence2' columns\n",
    "ks_stat, p_value = ks_2samp(df_text_train_with_emotional_means_modified['valence'], df_text_train_with_emotional_means_modified['valence2'])\n",
    "\n",
    "ks_stat, p_value\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Calculate the Cramér-von Mises distance for 'valence' and 'valence2' columns\n",
    "cv_distance, cv_p_value = wilcoxon(df_text_train_with_emotional_means_modified['valence'], df_text_train_with_emotional_means_modified['valence2'])\n",
    "\n",
    "cv_distance, cv_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Calculate the KS-test statistic and p-value for 'valence' and 'valence2' columns\n",
    "ks_stat, p_value = ks_2samp(df_text_train_with_emotional_means_modified['valence'], df_text_train_with_emotional_means_modified['valence2'])\n",
    "\n",
    "ks_stat, p_value\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Calculate the Cramér-von Mises distance for 'valence' and 'valence2' columns\n",
    "cv_distance, cv_p_value = wilcoxon(df_text_train_with_emotional_means_modified['valence'], df_text_train_with_emotional_means_modified['valence2'])\n",
    "\n",
    "cv_distance, cv_p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valence\n",
    "(0.8806998071294909, 0.9384560762920612, 0.046242530911141645, 0.704150943408302, 21.220555804250772)\n",
    "\n",
    "# Arousal\n",
    "(1.1862474465413284, 1.0891498733146547, 0.0067130068708339844, 0.8799446540815096, 24.117059208348085)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the KS-test statistic and p-value for 'arousal' and 'arousal2' columns\n",
    "ks_stat_arousal, p_value_arousal = ks_2samp(df_text_train_with_emotional_means_modified['arousal'], df_text_train_with_emotional_means_modified['arousal2'])\n",
    "\n",
    "# Calculate the Cramér-von Mises distance for 'arousal' and 'arousal2' columns\n",
    "cv_distance_arousal, cv_p_value_arousal = wilcoxon(df_text_train_with_emotional_means_modified['arousal'], df_text_train_with_emotional_means_modified['arousal2'])\n",
    "\n",
    "ks_stat_arousal, p_value_arousal, cv_distance_arousal, cv_p_value_arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "- Please perform Sentiment Classification task. Given the picture, assign a sentiment label from ['Very negative', 'Moderately negative', 'Somewhat negative','Neutral', 'Somewhat positive', 'Moderately positive', 'Very positive']. Return label only without any other text.\n",
    "  - (We will ask you to rate a series of pictures in terms of how positive or negative they make you feel.)\n",
    "- Please perform Arousal Classification task. Given the picture, assign a arousal label from ['Very low', 'Moderately low', 'Somewhat low','Neutral', 'Somewhat high', 'Moderately high', 'Very high']. Return label only without any other text.\n",
    "  - (We will ask you to rate a series of pictures in terms of how excited or aroused they make you feel. In other words, we would like to know how intense the feeling is that picture evokes; whether it is good or bad does not matter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded xlsx file\n",
    "file_path = 'img-GPT.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure that we only consider the first 30 rows\n",
    "data = data.head(30)\n",
    "\n",
    "# Extract the actual values ('pic_valence') and the predicted values ('GPT valence')\n",
    "actual_values = data['pic_valence']\n",
    "predicted_values = data['GPT valence']\n",
    "\n",
    "# Calculate the errors\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(actual_values, predicted_values)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Coefficient of Determination (R-squared)\n",
    "r2 = r2_score(actual_values, predicted_values)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(actual_values, predicted_values)\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((actual_values - predicted_values) / actual_values)) * 100\n",
    "\n",
    "mse, rmse, r2, mae, mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the Cramér-von Mises test call and extracting the statistics and p-value properly\n",
    "cv_result = cramervonmises(predicted_values, 'norm', args=(np.mean(predicted_values), np.std(predicted_values)))\n",
    "\n",
    "# Extracting the statistics and p-value from the Cramér-von Mises result object\n",
    "cv_stat = cv_result.statistic\n",
    "cv_pvalue = cv_result.pvalue\n",
    "\n",
    "ks_stat, ks_pvalue, cv_stat, cv_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for additional codes for experimental analysis, f-tests, and ANOVA analysis are included in EX01.ipynb, EX02.ipynb, and f-test.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Secondary data\n",
    "As previously mentioned, this transparency material does not require a detailed report on the data collection process. Due to the sensitive nature of the data and other confidentiality considerations, we are unable to disclose the intermediate datasets used in this study. However, to ensure the reproducibility and rigor of our research, we provide comprehensive documentation of the methods, computations, and code used for data processing and analysis in the subsequent section, *Quantitative Design/Computational Research*. This approach strikes a balance between respecting data privacy and adhering to the principles of transparency in scientific research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative design/computational research\n",
    "## Decision Tree\n",
    " As part of the additional analysis presented in this study, we employed decision tree methods to investigate the relative importance of various aesthetic features of images in influencing the perceived helpfulness of online reviews. This approach allowed us to systematically identify and rank the aesthetic attributes that contribute most significantly to the helpfulness ratings. By leveraging decision trees, we provided a clear, interpretable framework to assess the impact of these visual features, offering valuable insights into the role of image aesthetics in shaping user perceptions and engagement with online reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# grid search for model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data.loc[:,['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14','A15','A16']]\n",
    "y1 = data.loc[:,['PV5']]\n",
    "y2 = data.loc[:,['PA1']]\n",
    "\n",
    "# Train a decision tree regression model\n",
    "regr1 = DecisionTreeRegressor(max_depth=2,min_samples_leaf=0.05, min_samples_split=0.05)\n",
    "regr1.fit(X, y1)\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure()\n",
    "tree.plot_tree(regr1, filled=True,feature_names=X.columns,class_names=y1.columns)\n",
    "plt.show()\n",
    "\n",
    "regr2 = DecisionTreeRegressor(max_depth=2,min_samples_leaf=0.05, min_samples_split=0.05)\n",
    "regr2.fit(X, y2)\n",
    "plt.figure()\n",
    "tree.plot_tree(regr2, filled=True,feature_names=X.columns,class_names=y2.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabcut\n",
    "In our image processing pipeline, we employed the GrabCut algorithm to perform segmentation and identify the primary visual subjects within the images. GrabCut is an interactive foreground extraction method that utilizes iterated graph cuts for efficient image segmentation. \n",
    "\n",
    "One of the key advantages of GrabCut is its ability to minimize the energy function associated with segmentation, leading to more accurate and efficient extraction of the region of interest. \n",
    "\n",
    "By leveraging the capabilities of the GrabCut algorithm, we achieved precise segmentation of images, effectively isolating the visual subjects. This process was instrumental in our analysis, as it allowed for a focused examination of the aesthetic features pertinent to our study.\n",
    "\n",
    "The implementation of GrabCut in our methodology not only enhanced the accuracy of subject identification but also contributed to the overall efficiency of our image processing workflow. Its iterative approach and ability to preserve edge information ensured that the segmented outputs were of high quality, thereby supporting the robustness of our subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv.imread('XXX.jpg')\n",
    "assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
    "mask = np.zeros(img.shape[:2],np.uint8)\n",
    "\n",
    "bgdModel = np.zeros((1,65),np.float64)\n",
    "fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "rect = (0,0,699,524)\n",
    "cv.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv.GC_INIT_WITH_RECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG\n",
    "In selecting a deep learning model for our study, we utilized the VGG-16 architecture to predict the picture-evoked valence and arousal. VGG-16 is a convolutional neural network renowned for its depth and effectiveness in image classification tasks. It comprises 16 weight layers, including 13 convolutional layers and 3 fully connected layers, enabling it to capture intricate features within images. \n",
    "\n",
    "One of the notable advantages of VGG-16 is its high accuracy in image classification, achieving a top-5 test accuracy of 92.7% on the ImageNet dataset, which consists of over 14 million images across nearly 1,000 classes.  This high performance is attributed to its deep architecture, which allows for the learning of complex representations.\n",
    "\n",
    "Additionally, VGG-16’s architecture is straightforward and well-understood, making it a popular choice for transfer learning applications. Its pre-trained weights can be fine-tuned for specific tasks, enhancing performance even with limited training data. \n",
    "\n",
    "In the context of predicting valence and arousal, VGG-16 has been effectively utilized in previous studies. For instance, research has demonstrated its capability in extracting relevant features related to emotional states, facilitating accurate predictions of valence and arousal levels. \n",
    "\n",
    "By leveraging VGG-16’s robust feature extraction capabilities and adaptability through transfer learning, our study aimed to accurately model the emotional responses evoked by images, specifically in terms of valence and arousal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return K.mean(K.abs((y_true - y_pred) / K.clip(y_true, K.epsilon(), None)) * 100)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def normalize_target(target_values):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_values = target_values.reshape(-1, 1)\n",
    "    return scaler.fit_transform(target_values)\n",
    "\n",
    "def load_data(image_dir, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_paths = df['image_ID'].values\n",
    "    valence_values = df['valence'].values\n",
    "    arousal_values = df['arousal'].values\n",
    "    normalized_valence = normalize_target(valence_values)\n",
    "    normalized_arousal = normalize_target(arousal_values)\n",
    "    \n",
    "    X_data = []\n",
    "    for image_path in image_paths:\n",
    "        img_path = os.path.join(image_dir, image_path)\n",
    "        img = load_and_preprocess_image(img_path)\n",
    "        X_data.append(img)\n",
    "    \n",
    "    X_data = np.array(X_data)\n",
    "    return X_data, normalized_valence, normalized_arousal\n",
    "\n",
    "def create_model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))  \n",
    "    for layer in base_model.layers[:12]:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    valence_output = Dense(1, name='valence')(x)\n",
    "    arousal_output = Dense(1, name='arousal')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=[valence_output, arousal_output])\n",
    "    model.compile(optimizer=Adam(lr=0.0001), \n",
    "                  loss={'valence': rmse, 'arousal': rmse},\n",
    "                  metrics={'valence': mape, 'arousal': mape})\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train_valence, y_train_arousal, X_val, y_valence_val, y_arousal_val, model):\n",
    "    train = model.fit(X_train, \n",
    "                        {'valence': y_train_valence, 'arousal': y_train_arousal},\n",
    "                        validation_data=(X_val, {'valence': y_valence_val, 'arousal': y_arousal_val}),\n",
    "                        epochs=10, batch_size=32)\n",
    "    return train\n",
    "\n",
    "def main():\n",
    "    image_dir = 'images'\n",
    "    csv_file = 'NewDataset01.csv'\n",
    "    X_data, y_valence, y_arousal = load_data(image_dir, csv_file)\n",
    "    X_train, X_val, y_train_valence, y_valence_val, y_train_arousal, y_arousal_val = train_test_split(\n",
    "        X_data, y_valence, y_arousal, test_size=0.2, random_state=5)\n",
    "    model = create_model()\n",
    "    history = train_and_save_model(X_train, y_train_valence, y_train_arousal, X_val, y_valence_val, y_arousal_val, model)\n",
    "    \n",
    "    val_loss, val_mape_valence, val_mape_arousal = model.evaluate(X_val, {'valence': y_valence_val, 'arousal': y_arousal_val})\n",
    "    print(f'Validation MAPE (Valence): {val_mape_valence:.4f}%')\n",
    "    print(f'Validation MAPE (Arousal): {val_mape_arousal:.4f}%')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "In this study, we employ the BERT (Bidirectional Encoder Representations from Transformers) model for transfer learning to train a predictive model for text valence and arousal. BERT is a state-of-the-art deep learning model designed for natural language understanding. It utilizes a bidirectional transformer architecture, allowing it to consider the context of words from both the left and the right simultaneously, which is crucial for capturing nuanced meanings in text.\n",
    "\n",
    "One of the primary advantages of BERT lies in its pre-training on large-scale text corpora, such as the English Wikipedia and BookCorpus, using two unsupervised learning tasks: masked language modeling and next sentence prediction. This pre-training equips the model with a robust understanding of linguistic structures and semantic relationships, making it highly effective for downstream tasks, including sentiment analysis, emotion prediction, and other natural language processing applications.\n",
    "\n",
    "In the context of our study, BERT’s ability to generate rich, contextualized embeddings for textual data provides a solid foundation for modeling the valence and arousal. By leveraging transfer learning, we fine-tuned the pre-trained BERT model on our specific dataset, enabling it to adapt to the task of predicting emotional states in textual content. This approach not only enhances model performance but also reduces the computational resources and data requirements compared to training a model from scratch.\n",
    "\n",
    "BERT’s superior performance in various benchmarks, coupled with its flexibility and adaptability, makes it an ideal choice for our research. Its implementation ensures that our analysis is grounded in advanced, reliable, and interpretable methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % 1\n",
    "from xgboost import XGBClassifier\n",
    "from bert_serving.client import BertClient\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "tqdm.pandas(desc=\"Pandas\")\n",
    "\n",
    "# % sklearn\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# load annotated data\n",
    "a_d = './text_train.xlsx'\n",
    "df = pd.read_excel(a_d)\n",
    "\n",
    "# check comment length distribution and choose BERT's max seq length as 30\n",
    "# df.review_content.apply(lambda x: len(\n",
    "#     x) if isinstance(x, str)else 0).describe()\n",
    "'''\n",
    "count    1325.000000\n",
    "mean       83.387925\n",
    "std        79.893124\n",
    "min        13.000000\n",
    "25%        24.000000\n",
    "50%        52.000000\n",
    "75%       122.000000\n",
    "max       918.000000\n",
    "Name: review_content, dtype: float64\n",
    "'''\n",
    "df = df[df['review_content'].notna()]\n",
    "bc = BertClient()\n",
    "bert_emb = bc.encode(df['review_content'].values.tolist())\n",
    "bc.close()\n",
    "bert_emb.shape\n",
    "with open('./text_train.npy', 'wb') as outfile:\n",
    "    np.save(outfile, bert_emb)\n",
    "bert_emb0 = np.load('./text_train.npy')\n",
    "\n",
    "'''\n",
    "bert_emb1 = np.insert(bert_emb0, 768, df['fake'], axis=1)\n",
    "bert_emb2 = np.insert(bert_emb1, 769, df['environment'], axis=1)\n",
    "bert_emb3 = np.insert(bert_emb2, 770, df['service'], axis=1)\n",
    "data_a = np.array([df['taste']])\n",
    "data_b = np.insert(data_a.transpose(),1,df['service'],axis=1)\n",
    "data_c = np.insert(data_b,2,df['environment'],axis=1)\n",
    "data_d = np.insert(data_c,1,df['e_score'],axis=1)\n",
    "'''\n",
    "\n",
    "\n",
    "def get_clf_report(model):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    conmat = confusion_matrix(y_test, y_pred)\n",
    "    conmat = np.mat(conmat)\n",
    "    print(conmat)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return y_pred, conmat\n",
    "\n",
    "\n",
    "X = bert_emb0\n",
    "Y = df['val4'].astype(str)\n",
    "\n",
    "# train, test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "y_pred, conmat = get_clf_report(LogisticRegression(\n",
    "    solver='lbfgs', random_state=0, C=2, class_weight='balanced'))\n",
    "\n",
    "# SVC\n",
    "\n",
    "\n",
    "def svm_CV():\n",
    "    C = [0.1, 1, 10, 100]\n",
    "    gamma = [1, 0.1, 0.01]\n",
    "    kernel = ['rbf', 'linear']\n",
    "    random_grid = {'C': C, 'gamma': gamma, 'kernel': kernel}\n",
    "    model = SVC(random_state=0, class_weight='balanced')\n",
    "    svm_random = RandomizedSearchCV(estimator=model,\n",
    "                                    param_distributions=random_grid,\n",
    "                                    scoring='f1_weighted',\n",
    "                                    n_iter=20, cv=5,\n",
    "                                    verbose=2, random_state=0)\n",
    "    svm_random.fit(x_train, y_train)\n",
    "\n",
    "    return svm_random\n",
    "\n",
    "\n",
    "svm_random = svm_CV()\n",
    "# get best parameter\n",
    "parameters = svm_random.best_params_\n",
    "print(parameters)\n",
    "\n",
    "clf = SVC(random_state=0, class_weight='balanced', **parameters)\n",
    "y_pred, conmat = get_clf_report(clf)\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "\n",
    "def get_rf_CV():\n",
    "    n_estimators = [10, 50, 100]\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "    max_depth = [5, 10, 20, 50, None]\n",
    "    min_samples_split = [8, 16, 32]\n",
    "    min_samples_leaf = [8, 16, 32]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "    model = RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "    rf_random = RandomizedSearchCV(estimator=model,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=30, cv=5,\n",
    "                                   scoring='f1_weighted',\n",
    "                                   verbose=2,\n",
    "                                   random_state=0)\n",
    "    rf_random.fit(x_train, y_train)\n",
    "    return rf_random\n",
    "\n",
    "\n",
    "rf_random = get_rf_CV()\n",
    "# get best parameter\n",
    "parameters = rf_random.best_params_\n",
    "print(parameters)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0,\n",
    "                             class_weight='balanced',\n",
    "                             **parameters)\n",
    "y_pred, conmat = get_clf_report(clf)\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "\n",
    "def get_xgb_CV():\n",
    "    n_estimators = [10, 50, 100, 200, 500]\n",
    "    max_depth = [2, 5, 10, 20, 50, None]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_depth': max_depth}\n",
    "\n",
    "    model = XGBClassifier(random_state=0)\n",
    "    clf_random = RandomizedSearchCV(estimator=model,\n",
    "                                    param_distributions=random_grid,\n",
    "                                    n_iter=20, cv=5,\n",
    "                                    scoring='f1_weighted',\n",
    "                                    verbose=2,\n",
    "                                    random_state=0)\n",
    "    clf_random.fit(x_train, y_train)\n",
    "    return clf_random\n",
    "\n",
    "\n",
    "xgb_random = get_xgb_CV()\n",
    "# get best parameter\n",
    "parameters = xgb_random.best_params_\n",
    "print(parameters)\n",
    "\n",
    "clf = XGBClassifier(random_state=0,\n",
    "                    **parameters)\n",
    "y_pred, conmat = get_clf_report(clf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
